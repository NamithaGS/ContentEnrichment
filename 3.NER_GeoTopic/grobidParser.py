
import os
from tika import detector
from tika import parser
import subprocess
import re
import json
from tika.parser import _parse
from tika.tika import callServer, ServerEndpoint

# Path for directory containing files to be processed
path = "/Users/charanshampur/newAwsDump/testFiles2"

# Path for Google Scholar Api Program
googleScholarProgramPath="/Users/charanshampur/PycharmProjects/CSCI599/scholar.py"

# Python Interpreter to be used to run the Scholar API
pythonInterpreter="/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7"

# File containing metadata about related publication and geographic location
publicationFile = open("Publication.json","w")

# output file generated from TagRatioparser.py
measurementFile = open("Measurement.json","r")


measJson = json.load(measurementFile)

# Function for processing the metadata generated by running grobid journal parser
def processGorbid(metadata):
    meta={}
    if "Author" in metadata:
        meta["Author"]=metadata["Author"]
    if "grobid:header_Title" in metadata:
        meta["grobid:header_Title"]=metadata["grobid:header_Title"]
    if "grobid:header_Authors" in metadata:
        meta["grobid:header_Authors"]=metadata["grobid:header_Authors"]
    if "grobid:header_Affiliations" in metadata:
        meta["grobid:header_FullAffiliations"]=metadata["grobid:header_FullAffiliations"]
    if "grobid:header_Address" in metadata:
        meta["grobid:header_Address"]=metadata["grobid:header_Address"]
    if "title" in metadata:
        meta["title"]=metadata["title"]
    return meta

# Function for preparing the author list and phrase for fetching related publications
def getRelatedPub(metadata):
    pubGrobidAuthor=[]
    pubGrobidHeader=[]
    pubAuthor=[]
    if "grobid:header_Authors" in metadata:
        authors=metadata["grobid:header_Authors"].split(",")
        if(len(authors)!=0):
            for author in authors:
                if(re.match("\S",author)):
                    pubGrobidAuthor+=googleScholarApi(True,author)
                    if len(pubGrobidAuthor) > 40:
                        break
    if "grobid:header_Title" in metadata:
        phrase=metadata["grobid:header_Title"]
        pubGrobidHeader=googleScholarApi(False,phrase)
    if "Author" in metadata:
        authors=metadata["Author"].split(",")
        if(len(authors)!=0):
            for author in authors:
                if(re.match("\S",author)):
                    pubAuthor+=googleScholarApi(True,author)
                    if len(pubAuthor) > 40:
                        break
    pubAuthor = pubAuthor + pubGrobidAuthor + pubGrobidHeader
    return pubAuthor



# Function which calls the google scholar api
def googleScholarApi(authorFLag,data):
    pubList=[]
    data=data.encode('ascii', 'ignore')
    if authorFLag:
        buffer=subprocess.check_output([pythonInterpreter, googleScholarProgramPath, '-a', "'"+data+"'", '-c 20', '--csv'])
    else:
        buffer=subprocess.check_output([pythonInterpreter, googleScholarProgramPath, '-p', "'"+data+"'", '-c 20', '--csv'])
    if(buffer!=None and buffer!=""):
        listOfPub=buffer.split("...")
        for publication in listOfPub:
            if(publication!="\n"):
                details = publication.split("|")
                if(len(details)>2):
                    pubStr = "Title : "+details[0].strip()+", Year-Published : "+details[2].strip()+", URL : "+details[1].strip()
                    pubList.append(pubStr)
    return pubList

# function for selecting the metadata fetched from geotopic parser
def getGeoTags(geoMetadata,metaData):
    for k,v in geoMetadata.items():
        if re.match(r'(Optional)+|(Geographic)+',k,re.M):
            metaData[k]=v
    return metaData

# Main function
publicationDict={}
for path,dirs,files in os.walk(path):
    for file in files:
        if file not in ".DS_Store":
            parsedData=""
            path_to_file = path+"/"+str(file)
            print path_to_file
            docType = detector.from_file(path_to_file)
            metaData={}
            buffer=""
            if docType=="application/pdf":
                parsedData=parser.from_file(path_to_file,"http://localhost:9090")
                if parsedData["content"]!=None:
                    metaData=processGorbid(parsedData["metadata"])
                    buffer = parsedData["content"]
            else:
                if path_to_file not in measJson:
                    continue
                metaDataNer = measJson[path_to_file]
                if "NER_PERSON" in metaDataNer:
                    metaData["Author"]=''.join(metaDataNer["NER_PERSON"])
                if "content" in metaDataNer:
                    buffer=metaDataNer["content"]
                try:
                    metaData["title"]=parser.from_file(path_to_file,"http://localhost:9091")["metadata"]["title"]
                except:
                    pass
            metaDataPublication=getRelatedPub(metaData)
            if (len(metaDataPublication)>0):
                metaData["Publications"]=metaDataPublication
            if (len(buffer)>0):
                buffer=buffer.encode('ascii', 'ignore')
                status, response = callServer('put', ServerEndpoint, '/rmeta', buffer,
                    {'Content-Type' : 'application/geotopic'}, False)
                geoMetaData=_parse((status,response))
                if "metadata" in geoMetaData:
                    metaData=getGeoTags(geoMetaData["metadata"],metaData)
            if (len(metaData)>0):
                publicationDict[path_to_file]=metaData

json.dump(publicationDict,publicationFile,indent=4)
measurementFile.close()
publicationFile.close()